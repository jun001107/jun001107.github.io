[
  {
    "id": "1",
    "title": "Visual CoT: LLM Reasoning for Rush Hour Puzzles",
    "description": "This project investigates the spatial reasoning capabilities of Large Language Models (specifically Qwen2.5-7B) by training them to solve the 'Rush Hour' sliding block puzzle. The system uses a two-stage training pipeline: Supervised Fine-Tuning (SFT) using LoRA to teach the model board interpretation and output formatting, followed by Reinforcement Learning using Group Relative Policy Optimization (GRPO) to optimize for valid and optimal solutions. It includes a custom evaluation framework to parse and verify Chain-of-Thought (CoT) reasoning.",
    "tags": [
      "Large Language Models",
      "Reinforcement Learning",
      "GRPO",
      "Supervised Fine-Tuning",
      "LoRA",
      "Visual Reasoning",
      "Chain-of-Thought",
      "Hugging Face Transformers",
      "TRL"
    ],
    "image": "/photos/p1.png",
    "github": "#",
    "timeline": "5 Mos",
    "performance": "High",
    "role": "Researcher",
    "motivation": "The primary motivation is to explore whether LLMs can transcend simple text processing to handle complex visual-spatial planning tasks. By using Rush Hour—a game requiring look-ahead and constraint satisfaction—the project aims to determine if Reinforcement Learning (GRPO) can incentivize models to develop robust internal representations and reasoning strategies (Visual CoT) that outperform standard supervised baselines.",
    "challenges": "A significant challenge was designing a text-based representation of the 2D game board that allowed the model to accurately infer spatial relationships. Furthermore, defining an effective reward function for GRPO was critical; it had to balance penalties for syntax errors and invalid moves (e.g., collisions) with high rewards for optimal solutions. Parsing free-form natural language Chain-of-Thought output into executable moves for verification also posed a parsing and robustness challenge.",
    "learnings": "The project demonstrated that while SFT is effective for teaching models the rules and syntax of the game, Reinforcement Learning is crucial for discovering optimal strategies. Implementing the custom reward function highlighted how granular feedback (distinguishing between 'valid but unsolved' and 'invalid') significantly impacts convergence. Additionally, the project reinforced the utility of Parameter-Efficient Fine-Tuning (LoRA) for adapting large 7B models on consumer hardware."
  },
  {
    "id": "2",
    "title": "Production-Grade MLOps Recommendation Engine",
    "description": "This project re-architected a prototype collaborative filtering model into a robust, containerized microservice. It features a complete MLOps pipeline including automated data ingestion, scheduled model retraining, and a zero-downtime 'Canary' deployment strategy. The system is fully containerized using Docker and orchestrated via Docker Compose, with a rigorous CI/CD workflow managed through GitLab.",
    "tags": [
      "Python",
      "Flask",
      "Docker",
      "Kubernetes/Docker Compose",
      "CI/CD",
      "GitLab CI",
      "MLOps",
      "Nginx",
      "AWS S3",
      "Pytest",
      "Prometheus"
    ],
    "image": "/photos/p2.jpeg",
    "github": "#",
    "timeline": "5 Mos",
    "performance": "High",
    "role": "Engineer",
    "motivation": "The primary objective was to bridge the gap between Data Science and Software Engineering by applying production-grade rigor to an ML workload. The project aimed to replace manual, error-prone model updates with a fully automated pipeline that ensures reproducibility, reliability, and seamless scalability. A key focus was establishing a 'Zero-Downtime' deployment capability to update models without interrupting user service.",
    "challenges": "A significant engineering challenge was implementing the CI/CD pipeline on resource-constrained shared runners, which caused persistent 'Docker-in-Docker' and memory exhaustion issues during the build process. Additionally, designing the 'Canary Rollout' mechanism—which dynamically routes traffic between 'Stable' and 'Candidate' models using Nginx—required complex orchestration to ensure data consistency and transparent fallback mechanisms in case of model failure.",
    "learnings": "This project provided deep practical experience in the MLOps lifecycle, specifically in decoupling model training from service logic. I learned how to architect testable microservices (using Pytest for both unit and integration testing), manage artifact versioning with AWS S3, and configure automated deployment pipelines. It also reinforced the importance of observability, as implementing Prometheus and Grafana was essential for monitoring the health of the split-traffic deployments."
  },
  {
    "id": "3",
    "title": "Montreal Libraries Management System",
    "description": "This project is a comprehensive database application designed to manage the operations of public libraries in Montreal. It handles the full lifecycle of library resources, including book cataloging, author management, multi-branch inventory, citizen reservations, loan tracking, and automated fine calculation. The system is built on a relational database (IBM DB2) and features a modular Java application interface that interacts with the database via JDBC.",
    "tags": [
      "Java",
      "JDBC",
      "IBM DB2",
      "SQL",
      "Database Design",
      "DAO Pattern",
      "ER Modeling",
      "Stored Procedures",
      "Data Normalization"
    ],
    "image": "/photos/p3.jpg",
    "github": "#",
    "timeline": "3 Mos",
    "performance": "Medium",
    "role": "Researcher",
    "motivation": "The primary objective was to apply theoretical database concepts—such as Entity-Relationship modeling, normalization, and relational algebra—to a real-world scenario. The project aimed to solve the logistical challenge of tracking thousands of books across multiple branches while ensuring data integrity for critical user transactions like reservations and overdue fines. From a software engineering perspective, it served as a practical exercise in architecting a robust application layer that cleanly separates business logic from data access.",
    "challenges": "A significant design challenge was modeling complex relationships, such as the chain of weak entities (Fines depend on Loans, which depend on Copies, which depend on Books). Implementing these dependencies while maintaining referential integrity required rigorous SQL constraints. On the application side, managing JDBC resources was critical; ensuring that connections, statements, and result sets were properly closed to prevent memory leaks—while handling SQL exceptions gracefully—required a disciplined implementation of the 'try-with-resources' pattern and custom exception handling.",
    "learnings": "This project provided mastery over the Data Access Object (DAO) pattern, demonstrating how to decouple low-level SQL operations from high-level application logic. I learned how to implement the Factory pattern to manage database connections efficiently and ensure dependency injection across different handlers. Furthermore, writing Stored Procedures and designing Indexes highlighted the importance of pushing logic and optimization down to the database layer to improve performance and consistency."
  },
  {
    "id": "4",
    "title": "Distributed Travel Reservation System",
    "description": "This project transforms a monolithic travel reservation system into a distributed application with a central middleware and four distinct resource managers (Flights, Cars, Rooms, Customers). It features two implementation versions: one utilizing standard Java RMI and another built on a custom, non-blocking TCP RPC framework using Java Reflection and Virtual Threads.",
    "tags": [
      "Java",
      "Distributed Systems",
      "RPC",
      "RMI",
      "Middleware",
      "TCP/IP",
      "Concurrency",
      "Virtual Threads"
    ],
    "image": "/photos/p4.jpg",
    "github": "#",
    "timeline": "6 Mos",
    "performance": "High",
    "role": "Lead",
    "motivation": "The primary goal was to explore the architectural shift from monolithic to distributed systems to improve modularity and fault tolerance. Specifically, by manually implementing a custom RPC framework over raw TCP sockets, the project aimed to provide a deep comparative analysis between industry-standard abstractions (Java RMI) and low-level network programming, while also leveraging modern Java features like Virtual Threads for high-concurrency performance.",
    "challenges": "A major technical hurdle was maintaining data consistency across isolated resource managers without a shared database. This necessitated complex aggregation logic in the middleware for customer itineraries and a manual rollback mechanism to ensure atomicity during 'Bundle' transactions (simultaneous booking of flights, cars, and rooms). Additionally, designing a custom, type-safe RPC layer that mirrored RMI's ease of use required extensive implementation of dynamic proxies and Java Reflection.",
    "learnings": "This project reinforced key distributed system concepts, particularly the trade-off between service modularity and the complexity of data aggregation. It provided practical experience in implementing non-blocking servers using Java 21's Virtual Threads to handle concurrent client requests efficiently. Furthermore, building the RPC framework from scratch deepened the understanding of serialization, reflection, and protocol design."
  },
  {
    "id": "5",
    "title": "Distributed Consensus with Paxos",
    "description": "This project implements the Paxos consensus algorithm to enforce total order broadcast for 'Treasure Island', a distributed multiplayer game. The system ensures that all game instances process player moves in the exact same sequence, maintaining a consistent global state (state machine replication) across all replicas despite concurrency, network latency, and process failures.",
    "tags": [
      "Java",
      "Paxos Algorithm",
      "Distributed Consensus",
      "Total Order Broadcast",
      "Fault Tolerance",
      "Concurrency",
      "State Machine Replication"
    ],
    "image": "/photos/p5.png",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "High",
    "role": "Developer",
    "motivation": "The primary objective was to bridge the gap between the theoretical concept of consensus and practical implementation. By building a Paxos module from scratch, the project aimed to solve the challenge of replicating state in an asynchronous distributed system where processes may fail or messages may be delayed. It served as a rigorous exercise in ensuring data consistency and implementing robust fault-tolerance mechanisms without relying on a central coordinator.",
    "challenges": "A major technical challenge was implementing 'Multi-Paxos' to handle a continuous stream of game moves (log positions) rather than a single value agreement. Handling high contention was particularly difficult; when multiple players attempted to move simultaneously, they competed for the same log index, necessitating efficient retry logic and random backoff strategies to prevent livelock. Furthermore, ensuring the system remained safe (no conflicting commits) and live (progress is eventually made) in the face of injected failures—such as a leader crashing immediately after sending a proposal—required strict adherence to the protocol's quorum rules and careful state management.",
    "learnings": "This project provided a deep understanding of the two-phase commit structure of Paxos (Prepare/Promise and Propose/Accept) and the critical role of quorums in distributed agreement. I gained practical experience in handling asynchronous event-driven programming using Java's `CompletableFuture` and concurrent collections to manage proposer and acceptor states. Additionally, the project highlighted the complexities of debugging distributed systems, where non-deterministic message ordering and partial failures can lead to subtle race conditions."
  },
  {
    "id": "6",
    "title": "Distributed Task Execution Platform with ZooKeeper",
    "description": "This project implements a distributed computing platform that dynamically balances computational tasks across a cluster of worker nodes. It utilizes a leader-worker architecture where a central Manager orchestrates job assignments to Worker processes, with all coordination entirely decoupled and managed through Apache ZooKeeper. The system features a reactive, event-driven design using Watchers to eliminate polling and ensure system liveness.",
    "tags": [
      "Java",
      "Apache ZooKeeper",
      "Distributed Systems",
      "Load Balancing",
      "Event-Driven Architecture",
      "Leader Election",
      "Concurrency"
    ],
    "image": "/photos/p6.png",
    "github": "#",
    "timeline": "3 Mos",
    "performance": "High",
    "role": "Engineer",
    "motivation": "The primary goal was to architect a robust distributed system that avoids direct communication between nodes, relying instead on a shared coordination service. The project served as a deep dive into ZooKeeper synchronization primitives, specifically leveraging ephemeral nodes for failure detection and sequential nodes for FIFO task ordering to build a scalable and fault-tolerant computing cluster.",
    "challenges": "A major design challenge was synchronizing state between the Manager and Workers without relying on inefficient polling loops or encountering the 'thundering herd' problem. Implementing a strict time-slicing mechanism to prevent resource starvation was also difficult; it required handling `InterruptedException` gracefully to serialize intermediate computation states and allow interrupted tasks to be resumed by other workers without data loss.",
    "learnings": "This project reinforced the importance of event-driven programming in distributed systems, demonstrating how ZooKeeper Watchers can efficiently trigger state transitions. We gained practical experience in implementing leader election, managing distributed queues, and handling node churn. Additionally, the parallel development approach highlighted different architectural strategies for concurrency, leading to a unified, robust solution that handles edge cases like worker interruptions and task recovery."
  },
  {
    "id": "7",
    "title": "Natural Language Inference with PyTorch",
    "description": "This project involves building an end-to-end machine learning pipeline using PyTorch to solve a Natural Language Inference (NLI) task. The system predicts whether a 'premise' sentence entails a 'hypothesis' sentence. Key components include a custom data loader for batching and shuffling, text preprocessing (tokenization and indexing), and the implementation of three distinct model architectures: a Pooled Logistic Regression baseline, a Shallow Neural Network, and a Deep Neural Network.",
    "tags": [
      "Python",
      "PyTorch",
      "Natural Language Processing",
      "Machine Learning",
      "Neural Networks",
      "Deep Learning",
      "Data Preprocessing"
    ],
    "image": "/photos/p7.png",
    "github": "#",
    "timeline": "4 Mos",
    "performance": "High",
    "role": "Developer",
    "motivation": "The primary objective was to master the fundamentals of PyTorch by building ML models from scratch without relying on high-level abstractions. The project aimed to bridge the gap between theoretical concepts—such as backpropagation, loss calculation, and embeddings—and their practical implementation in code, specifically within the context of NLP.",
    "challenges": "A significant challenge was implementing core components 'from scratch' rather than using built-in library functions. This included designing a custom generator-based data loader (`build_loader`) instead of using PyTorch's `DataLoader`, and manually implementing the Binary Cross Entropy loss (`bce_loss`) and F1 score metrics using basic tensor operations. Additionally, managing variable-length sequences through padding and ensuring correct tensor shapes during the embedding, max-pooling, and concatenation stages required precise dimension handling.",
    "learnings": "This assignment provided deep insights into the inner workings of neural network training loops, including the specific mechanics of the forward and backward passes, optimizer steps, and gradient zeroing. I learned how to efficiently manage dynamic computation graphs using `nn.Module` and `nn.ModuleList` for deep networks. Furthermore, manual implementation of the data pipeline reinforced the importance of efficient batching strategies and tensor manipulation in handling text data."
  },
  {
    "id": "8",
    "title": "Word Embeddings and Bias Mitigation",
    "description": "This project focuses on the fundamental algorithms of Natural Language Processing and the ethical implications of learned representations. It involves the implementation of Word2Vec models (CBOW and Skip-Gram) from scratch using PyTorch, followed by a rigorous analysis of gender bias in pre-trained GloVe embeddings. The project culminates in the implementation of the 'HardDebias' technique to geometrically identify and remove gender stereotypes from the vector space.",
    "tags": [
      "Python",
      "PyTorch",
      "NLP",
      "Word2Vec",
      "CBOW",
      "Skip-Gram",
      "GloVe",
      "Bias Mitigation",
      "PCA",
      "WEAT"
    ],
    "image": "/photos/p8.png",
    "github": "#",
    "timeline": "3 Mos",
    "performance": "Medium",
    "role": "Developer",
    "motivation": "The primary motivation was to move beyond simply utilizing pre-trained models to understanding the mechanics of how semantic meaning is encoded in vectors. Furthermore, the project aimed to address the critical issue of algorithmic fairness, providing hands-on experience in quantifying societal biases (using metrics like DirectBias and WEAT) and applying mathematical interventions to mitigate them.",
    "challenges": "A significant technical challenge was correctly implementing the data preprocessing pipelines for CBOW and Skip-Gram, specifically handling the sliding window logic to generate context-target pairs. Additionally, implementing the 'HardDebias' algorithm required a deep understanding of linear algebra to correctly estimate the gender subspace using PCA and perform vector projections to orthogonalize embeddings without destroying their semantic utility.",
    "learnings": "This project provided a deep understanding of the distributional hypothesis and neural probabilistic language models. I learned how to manipulate high-dimensional vector spaces to isolate and remove specific semantic directions. It also reinforced the importance of statistical testing in NLP, specifically through the implementation of the Word Embedding Association Test (WEAT) to prove the significance of detected biases."
  },
  {
    "id": "9",
    "title": "Open-Domain QA & DistilBERT Fine-Tuning",
    "description": "This project explores advanced NLP techniques using PyTorch and Hugging Face Transformers across three distinct modules. It begins with fine-tuning a pre-trained DistilBERT model for Natural Language Inference (NLI) classification. It then implements 'Soft Prompting' (Prompt Tuning), a parameter-efficient fine-tuning method. The final and most complex module involves building a Dense Passage Retrieval (DPR) system for Open-Domain Question Answering, utilizing bi-encoders, in-batch negative sampling, and contrastive loss to retrieve relevant answers from a knowledge base.",
    "tags": [
      "Python",
      "PyTorch",
      "Hugging Face Transformers",
      "DistilBERT",
      "Natural Language Inference",
      "Prompt Tuning",
      "Open-Domain QA",
      "Dense Passage Retrieval",
      "Contrastive Learning",
      "Bi-Encoder"
    ],
    "image": "/photos/p9.png",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "Medium",
    "role": "Backend Dev",
    "motivation": "The primary goal was to bridge the gap between building models from scratch and leveraging industry-standard pre-trained Transformers. The project aimed to compare different transfer learning strategies—specifically full fine-tuning versus parameter-efficient prompt tuning. Furthermore, implementing the Open-Domain QA system was driven by the need to understand modern semantic search architectures (DPR) and how to train dual-encoders to match questions with answers in a dense vector space.",
    "challenges": "A major technical hurdle was correctly implementing the in-batch negative sampling for the Open-Domain QA system; this required efficient matrix operations to compute similarities between all questions and passages in a batch to serve as negative examples. Another significant challenge was the implementation of Soft Prompting, which involved freezing the underlying model parameters and surgically prepending learnable prompt vectors to the input embeddings while correctly adjusting attention masks to account for the extended sequence length.",
    "learnings": "This project provided hands-on mastery of the Hugging Face ecosystem, specifically how to wrap pre-trained models in custom PyTorch modules. I learned the mechanics of Contrastive Loss for training bi-encoders and how to evaluate retrieval systems using metrics like Recall@K and Mean Reciprocal Rank (MRR). Additionally, implementing Prompt Tuning offered deep insights into how large language models can be adapted to downstream tasks with minimal parameter updates."
  },
  {
    "id": "10",
    "title": "Comparative Analysis of KNN and Decision Trees",
    "description": "This project involves the manual implementation and comparative analysis of two fundamental machine learning algorithms: K-Nearest Neighbors (KNN) and Decision Trees. Applied to the Heart Disease (binary classification) and Penguin (multi-class classification) datasets, the study evaluates model performance based on accuracy, AUROC, and computational efficiency.",
    "tags": [
      "Python",
      "Machine Learning",
      "K-Nearest Neighbors",
      "Decision Trees",
      "Data Preprocessing",
      "Algorithm Implementation",
      "Model Evaluation"
    ],
    "image": "/photos/p10.jpg",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "Medium",
    "role": "Data Scientist",
    "motivation": "The primary objective was to gain a deep, low-level understanding of supervised learning algorithms by implementing them from scratch without relying on libraries like Scikit-Learn. The project aimed to empirically investigate the trade-offs between distance-based and rule-based classifiers, specifically examining how they perform across different data distributions (binary vs. multi-class) and feature sets.",
    "challenges": "Implementing the Decision Tree algorithm was technically demanding due to the complexity of managing recursive node splitting and implementing different cost functions (Gini Index vs. Entropy) manually. For KNN, balancing computational efficiency with accuracy was a challenge, as the algorithm's prediction time scales with the dataset size. Additionally, handling missing values and ensuring fair comparisons through proper hyperparameter tuning (e.g., K-value, Tree Depth) required rigorous experimentation.",
    "learnings": "This project demonstrated that while Decision Trees provide superior interpretability, KNN yielded higher accuracy (approx. 3.3% higher) for these specific datasets, proving effective where distance metrics align well with data clusters. I learned that data quality and feature relevance often outweigh dataset size, as the smaller Penguin dataset with distinct biological features yielded better models than the more complex Heart Disease dataset."
  },
  {
    "id": "11",
    "title": "Linear and Logistic Regression Analysis",
    "description": "This project involves the manual implementation and comparative evaluation of Linear Regression and Logistic Regression algorithms for both binary and multiclass classification tasks. Applied to the Breast Cancer Wisconsin and Wine Recognition datasets, the study analyzes the distinct behaviors of minimizing Sum of Squared Errors (SSE) versus Cross-Entropy loss, specifically examining feature importance rankings and decision boundary effectiveness.",
    "tags": [
      "Python",
      "Machine Learning",
      "Linear Regression",
      "Logistic Regression",
      "Gradient Descent",
      "Multiclass Classification",
      "Feature Engineering",
      "NumPy"
    ],
    "image": "/photos/p11.jpeg",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "Medium",
    "role": "Data Scientist",
    "motivation": "The primary objective was to deconstruct the mathematical foundations of classification by building models from scratch rather than relying on pre-built libraries. The project aimed to empirically demonstrate the theoretical superiority of probabilistic modeling (Logistic Regression) over linear decision boundaries for classification, while also exploring how different loss functions influence feature selection and model interpretability.",
    "challenges": "A significant technical challenge was optimizing the Gradient Descent algorithm for Logistic Regression, which required rigorous manual tuning of the learning rate and iteration limits to ensure convergence without divergence or overfitting. Additionally, reconciling the divergent feature importance rankings produced by the two models was complex; Linear Regression prioritized features capturing high variance, whereas Logistic Regression emphasized features that maximized class separability, requiring careful analytical interpretation.",
    "learnings": "This project solidified the understanding that while Linear Regression offers a computationally efficient closed-form solution, it is fundamentally ill-suited for classification compared to the probabilistic approach of Logistic Regression. I gained practical experience in deriving and implementing analytical gradients for backpropagation and learned that feature relevance is model-dependent—meaning the 'most important' features change depending on whether the objective is to fit data points or separate classes."
  },
  {
    "id": "12",
    "title": "Kuzushiji-MNIST Image Classification",
    "description": "This project involves the implementation and comparative analysis of three neural network architectures for classifying handwritten Japanese characters (Kuzushiji-MNIST). It features a Multilayer Perceptron (MLP) built entirely from scratch using NumPy to understand low-level backpropagation, alongside Convolutional Neural Networks (CNN) and Residual Networks (ResNet) implemented with high-level deep learning libraries to benchmark performance on complex visual data.",
    "tags": [
      "Python",
      "NumPy",
      "Deep Learning",
      "Computer Vision",
      "MLP",
      "CNN",
      "ResNet",
      "Image Classification",
      "Kuzushiji-MNIST",
      "Hyperparameter Tuning"
    ],
    "image": "/photos/p12.png",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "Medium",
    "role": "Data Scientist",
    "motivation": "The primary goal was to transcend the simplicity of standard MNIST by tackling the more challenging KMNIST dataset, which features character ambiguity and degradation. The project aimed to bridge the gap between theory and practice by manually implementing neural network fundamentals (forward/backward propagation, gradient descent) while empirically demonstrating why modern architectures like CNNs and ResNets are superior for spatial pattern recognition.",
    "challenges": "A key technical challenge was implementing the MLP backpropagation algorithm and gradient checking from scratch without bugs. Experimentally, the project faced the limitation of MLPs in capturing spatial hierarchies, which led to lower accuracy compared to CNNs. Additionally, tuning regularization was difficult; specifically, applying L2 regularization to the MLP resulted in underfitting, indicating that the model's capacity was already constrained enough relative to the dataset complexity.",
    "learnings": "The project demonstrated that while MLPs can learn basic representations, they fail to capture the spatial dependencies inherent in images, which CNNs handle efficiently through convolutional filters. I learned that residual connections in ResNets significantly mitigate optimization issues like vanishing gradients, allowing for deeper effective networks. Furthermore, the experiments highlighted that 'standard' improvements like L2 regularization are not universally beneficial and must be adapted to the specific model capacity and data distribution."
  },
  {
    "id": "13",
    "title": "BERT Text Classification on AG News",
    "description": "This project explores the capabilities of Large Language Models (LLMs) by implementing and evaluating BERT (Bidirectional Encoder Representations from Transformers) on the AG News text classification dataset. It involves two primary approaches: 'probing', where pre-trained BERT embeddings are used as features for traditional classifiers (KNN, Logistic Regression, MLP, Random Forest), and 'fine-tuning', where the entire BERT model is updated for the specific classification task.",
    "tags": [
      "Python",
      "PyTorch",
      "Hugging Face Transformers",
      "BERT",
      "NLP",
      "Text Classification",
      "Transfer Learning",
      "Fine-Tuning",
      "Feature Extraction",
      "AG News"
    ],
    "image": "/photos/p13.jpg",
    "github": "#",
    "timeline": "2 Mos",
    "performance": "Medium",
    "role": "Data Scientist",
    "motivation": "The primary objective was to understand the paradigm shift in Natural Language Processing from training models from scratch to the pre-training and fine-tuning methodology. The project aimed to empirically compare the effectiveness of using BERT purely as a feature extractor versus fine-tuning it end-to-end, thereby evaluating how deep contextual embeddings improve performance over traditional text processing techniques.",
    "challenges": "A significant challenge was managing the computational resources required for fine-tuning a large model like BERT. Additionally, analyzing the trade-offs between different pooling strategies (e.g., using the `[CLS]` token embedding vs. the mean of the last hidden state) for feature extraction required careful experimentation. Interpreting the results was also complex, as traditional classifiers trained on BERT embeddings performed surprisingly competitively against the fully fine-tuned model.",
    "learnings": "This project demonstrated the power of transfer learning, showing that pre-trained embeddings capture rich semantic information that can be leveraged effectively even by simple classifiers. I learned how to use the Hugging Face `transformers` library for tokenization and model loading, and gained insight into the internal mechanics of BERT, such as attention heads and embedding layers. Furthermore, the experiments highlighted that while fine-tuning typically yields the best results, feature extraction can be a highly efficient and effective alternative."
  }
]
